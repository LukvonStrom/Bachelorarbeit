\chapter{Modellierung}\label{chapter:Modellierun}
In diesem Kapitel sollen die Anforderungen aus den Interviews praktisch erhoben, die Referenzarchitekturen erstellt und die konstruierten Referenzarchitekturen folgend auch verglichen werden.
\section{Anforderungserhebung}\label{section:Anforderungserhebun}
\input{content/05-modellierung/1-anforderungserhebung}

\section{Echtzeitverarbeitung}\label{chap:ra-rt}
\input{content/05-modellierung/2-echtzeitverarbeitung}

\section{Batch-Verarbeitung}\label{chap:ra-batch}
\input{content/05-modellierung/3-batch}

\section{Einsatzszenarien der Referenzarchitekturen}\label{section:Einsatzszenarien-der-Referenzarchitekture}
Ob die Referenzarchitekturen im Tagesgeschäft eingesetzt werden können, wird anhand der Qualitätskriterien für Referenzarchitekturen überprüft. Aus diesem Grund wurden in  \anhangref{anhang:interview-philipp-03.05.2021}, \anhangref{anhang:interview-viet-04.05.2021} und \anhangref{anhang:interview-jan-04.05.2021} Interviews mit diversen Stakeholdern der Cloud Entwicklung geführt. Dabei war eine Verständlichkeit trotz der unterschiedlichen technischen Hintergründe der Interviewpartner zu erkennen. Ebenfalls haben die Stakeholder die Referenzarchitekturen akzeptiert und sahen die Qualität als zufriedenstellend an. Aufgrund des Feedbacks aus \anhangref{anhang:interview-viet-04.05.2021} wurden zwischen dem Stand zur Zeit des Interviews und dem finalen Stand Diagramme für bessere Verständlichkeit aufgeteilt. Dies dient auch der Behebung des Zuordnungsproblems aus \anhangref{anhang:interview-jan-04.05.2021}.

Eine Zugänglichkeit und ein Zugriff durch die Mehrheit der Organisation ist auf zweierlei Arten sichergestellt: Zum einen wird die fertige Bachelorarbeit an mehreren Stellen in den internen Wissensmanagementsystemen wie Confluence publiziert und zum anderen ist die Bachelorarbeit auf GitHub open source verfügbar.\footnote{Siehe \url{https://github.com/LukvonStrom/Bachelorarbeit}} Eine Wartbarkeit ist durch den offenen Prozess des Forkens via dem Versionsverwaltungstool Git und der offenen MIT-Lizenz, wie auch in \anhangref{anhang:interview-philipp-03.05.2021} erläutert, gewährt. Die Hauptprobleme dieser Domäne sind insofern adressiert, dass die Referenzarchitekturen mit den $\kappa$ und \ac{OLAP} Patterns auf etablierten Mustern basieren. Wertschöpfung für den Betrieb ist insofern garantiert, dass der Bedarf der Behandlung des Themas, wie in \anhangref{anhang:interview-philipp-03.05.2021} erläutert, aus dem Geschäftsalltag entstanden ist.


Im Folgenden soll beleuchtet werden, wofür sich welche der beiden entwickelten Referenzarchitekturen besser eignet.
Für das Monitoring eignet sich die in \autoref{chap:ra-rt} beschriebene Referenzarchitektur besser, da eine enge Integration sowohl mit CloudWatch Metrics als auch CloudWatch Logs besteht. So lassen sich Analysen zu in der \ac{AWS}-Cloud laufenden Workloads einfach durchführen. Dadurch, dass keine eigene Software geschrieben werden muss, um die Metriken in das Analysesystem zu laden, ist dieser Usecases wesentlich günstiger und weniger aufwändig abgedeckt.

Beide Referenzarchitekturen integrieren sich gut mit \AWSIOT{} Core und besitzen damit eine wichtige Voraussetzung zur Analyse der \ac{IoT} Daten. Gemäß der in \autoref{chap:datenwert} erläuterten Datenhalbwertszeit ist eine Auswahlentscheidung auch in Abhängigkeit von den Analyseanforderungen der Share- und Stakeholder der instanziierenden Architektur zu treffen. Unter Einsatz einer Dashboardinglösung wie QuickSight oder Grafana können historische Daten, wenn die Datenablage in \ac{S3} erfolgt, bei beiden Referenzarchitekturen auch Entscheidern zugänglich gemacht werden. 

Sollten längere Aufbewahrungsfristen der Daten über 30 Tage hinaus gewünscht sein, ist die Batch Architektur zu empfehlen, da die Datenhaltung großer Mengen an historischen Daten zu Analysezwecken möglich ist. Alternativ könnten beide Architekturen auch, ähnlich zur $\lambda$-Architektur kombiniert werden, um Timestream als Aufbewahrungslösung für ältere Daten bei Bedarf zu verwenden und Analysen über Echtzeitdaten mittels Kinesis durchzuführen. Bei einer solchen Kombination wäre zu beachten, die Speicherklasse \ac{HDD} zu verwenden, da die Latenz von Abfragen dann weniger kritisch ist. Wenn eine intervallbasierte Auswertung tolerierbar ist, ist Timestream zu verwenden, da die Gesamtkosten abhängig von der Art der Abfragen und die Komplexität der Architektur niedriger, als bei der Echtzeitreferenzarchitektur sind.


Da die Referenzarchitekturen als verteilte Systeme mit mehreren Diensten diverse Fehlerquellen haben können, die sich dann symptomatisch in den gezeigten Metriken bemerkbar machen, sind genaue Tests zum Verständnis der Fehlerzustände wichtig. Im Rahmen des \textit{Chaos Engineerings} lassen sich gezielt Fehler in das System einführen um die Resilienz gegenüber diversester Fehlerquellen zu testen.\footcite[Vgl.][]{Augsten.2020} Bei den vorgestellten Referenzarchitekturen ist es zu empfehlen, regelmäßige Chaos Experimente durchzuführen um so gezielt Fehlerszenarien wie doppelte Nachrichten, hohe Latenzen zwischen Diensten oder die temporäre Nichtverfügbarkeit einzelner Dienste zu simulieren und gezielt den Einfluss messen zu können. Daraus folgend können Erkentnisse für den besseren Betrieb einer resilienten Dateninfrastruktur abgeleitet und dokumentiert werden. Für \ac{AWS} gibt es den Dienst Fault Injection Simulator, welcher Fehler in Schnittstellen zwischen Diensten erzeugen kann und gezielt auch einzelne Infrastrukturkomponenten beeinträchtigen kann.\footcite[Vgl.][]{Barr.2021b} Unter Verwendung dieses Dienstes ist es möglich, die beschriebenen Chaos Experimente durchzuführen.

In den vorliegenden Referenzarchitekturen wurde gemäß \autoref{abb:DimensionenUebersicht} eine \enquote{so tief wie notwendige} Dekomposition erreicht, indem Variationspunkte mit starken Auswirkungen in stärkerer Detailtiefe erklärt werden, während beide Referenzarchitekturen auf den selben Dekompositionssichten aufbauen.

Eine hohe Anwendbarkeit wurde von den diversen Stakeholdern wie oben beschrieben in \anhangref{anhang:interview-philipp-03.05.2021}, \anhangref{anhang:interview-viet-04.05.2021} und \anhangref{anhang:interview-jan-04.05.2021} attestiert.
In Sachen Allgemeingültigkeit wurde insofern ein leicht abgesenkter Standard erreicht, da sich aufgrund der Prioritäten des Dienstvergleiches, die durch wichtige Stakeholder durchgeführt wurden eine für die Zielorganisation spezifische Architektur ergeben hat. Andere Organisationen, die beispielsweise eine Übertragbarkeit zwischen Clouds als wichtiger empfinden, hätten aufgrund eines anders priorisierten Vergleiches möglicherweise alternative Dienste wie OpenSearch in der Batchverarbeitung gewählt.

Innerhalb dieser Arbeit wurden Referenzarchitekturen für die Verarbeitung in der Cloud konstruiert. Ein Trend der dabei ausgespart wurde, weil sich wichtige Komponenten nicht in der Cloud befinden, ist das sogenannte \textit{Fog computing}. Nach der Definition von \citeauthor{Vaquero.2014} ist \textit{Fog computing} ein Szenario, in dem heterogene, allgegenwärtige und dezentralisierte Geräte kommunizieren und kooperieren um Speicher- und Verarbeitungsaufgaben zu übernehmen.\footcite[Vgl.][30\psq]{Vaquero.2014} In der Praxis führt dies dazu, dass Verarbeitungsaufgaben in Teilen, angelehnt an das \textit{Edge computing} von der Cloud in Richtung der Geräte ausgelagert wird.\footcite[Vgl.][]{Bonomi.2012} Dies geschieht dabei beispielsweise an Netzwerkgateways, die sowieso mit der Cloud kommunizieren und folgend nur noch bereits ausgewertete Daten übertragen. \ac{AWS} bietet mit dem Dienst Greengrass bereits eine Softwareplattform für \textit{Fog computing} an, die auf diversen qualifizierten Gateways läuft.\footcite[Vgl. auch im Folgenden][]{AmazonWebServicesInc..o.J.bu}